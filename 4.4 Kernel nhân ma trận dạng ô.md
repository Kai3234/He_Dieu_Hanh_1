# 1. Mục tiêu:
Hiểu cách nhân ma trận dạng dạng [[ô|ô nhớ]] [[Kernel]] : 
- Tải và sử dụng [[ô|ô nhớ]] để nhân ma trận.
- [[Đồng bộ rào cản]] và [[bộ nhớ chia sẻ]].
- Cân nhắc về tài nguyên.
- Đơn giản hóa bằng việc giả sử chiều rộng là bội số của kích thước [[ô|ô nhớ]].

# 2. Thuật toán nhân ma trận dạng [[ô|ô nhớ]]:
### **Ý tưởng chính**:
#### Nhân ma trận dạng ô nhớ nhằm:
- Chia ma trận lớn thành các ma trận nhỏ hơn (các [[ô]]) vừa với **[[bộ nhớ chia sẻ]] (shared memory)** của [[GPU]].
    
- Tăng cường khả năng **tái sử dụng dữ liệu** bằng cách lưu trữ tạm thời các phần dữ liệu cần thiết trong [[bộ nhớ chia sẻ]], tránh phải truy cập [[bộ nhớ toàn cục]] (global memory) nhiều lần.
### **Giả sử**: 
- Có hai ma trận đầu vào là **M**, **N** và ma trận kết quả là **P**. Để tận dụng tính song song và hiệu quả của [[bộ nhớ chia sẻ]], ta chia các ma trận thành các [[ô|ô nhớ]] với kích thước cố định, thường là `TILE_WIDTH x TILE_WIDTH`. Mỗi block của [[CUDA]] sẽ đảm nhận việc xử lý một [[ô|ô nhớ ]]của kết quả.

### **Các bước thực hiện thuật toán**:
> Khai báo shared memory để lưu ô nhớ của M và N:

![[Pasted image 20250324154919.png]]

> Xác định chỉ số hàng và cột của luồng trong ma trận P:

![[Pasted image 20250324155024.png]]

// trong đó: 
- `blockIdx.y` **và** `blockIdx.x`: Chỉ số của [[khối]] trong grid theo chiều dọc (y) và ngang (x). Mỗi [[khối]] trong grid đại diện cho một “[[ô|ô nhớ]]” của ma trận.
- `blockDim.y` **và** `blockDim.x`: Kích thước của [[khối]] theo chiều dọc và ngang (số lượng [[luồng]] trong [[khối]] theo từng chiều).
- `threadIdx.y` **và** `threadIdx.x`: Chỉ số của [[luồng]] bên trong [[khối]] theo hàng và cột. xác định vị trí của [[luồng]] trong [[ô|ô nhớ]].

//giải thích:
- Khi nhân `blockIdx` với `blockDim`, chúng ta "dịch chuyển" đến phần tử đầu tiên của [[khối]] ([[ô|ô nhớ]]) trong ma trận. 
- Sau đó cộng thêm `threadIdx` để xác định vị trí chính xác bên trong [[ô|ô nhớ]].

>  khởi tạo giá trị kết quả của phần tử P\[Row]\[Col]

![[Pasted image 20250324155140.png]]
	
>Vòng lặp qua tất cả các ô nhớ cần thiết để tính P\[Row]\[Col]:

![[Pasted image 20250324155945.png]]

// giải thích công thức:
- `Row * Width`: 
	- Tính toán vị trí bắt đầu của toàn bộ hàng có chỉ số `Row` trong mảng tuyến tính lưu trữ ma trận M.
	- **Ví dụ:** Nếu `Width = 8` và `Row = 2`, thì `Row * Width = 2 * 8 = 16` nghĩa là hàng thứ 2 bắt đầu tại chỉ số thứ 16 của mảng.
- `p * TILE_WIDTH + threadId.x`**:**
	- Xác định vị trí cột cho [[ô|ô nhớ]] hiện tại.
- kết hợp lại :
	- **Lấy phần tử ở hàng** `Row` **và cột** `(p * TILE_WIDTH + tx)` của ma trận M.
- tương tự với công thức ma trận N.

> Tính toán kết quả từng phần:

![[Pasted image 20250324155646.png]]

> Ghi kết quả vào ma trận P: 

![[Pasted image 20250324155904.png]]

## Phân tích thuật toán: 

- ==**Ưu điểm:**==
    - **Tăng hiệu quả truy cập bộ nhớ:** [[bộ nhớ chia sẻ||Shared memory ]]nhanh hơn [[bộ nhớ toàn cục||global memory]], do đó việc lưu trữ tạm các [[ô||ô nhớ]] trong [[bộ nhớ chia sẻ||shared memory]] giúp giảm số lần truy cập [[bộ nhớ toàn cục||global memory]].
        
    - **Tối ưu hóa song song:** Mỗi [[khối luồng|thread block]] có thể tính toán độc lập một [[ô|ô nhớ]], tận dụng tối đa khả năng [[tính toán song song]] của [[GPU]].
        
    - **Giảm băng thông [[bộ nhớ]]:** [[dữ liệu]] được tái sử dụng trong [[bộ nhớ chia sẻ|shared memory]] thay vì truy cập lại từ [[bộ nhớ toàn cục|global memory]].
        
- ==**Hạn chế:**==
    - **Giới hạn kích thước [[bộ nhớ chia sẻ|shared memory]]:** Kích thước TILE_WIDTH phải được chọn sao cho không vượt quá dung lượng [[bộ nhớ chia sẻ|shared memory]] của [[GPU]].
        
    - Các lệnh `__syncthreads()` cần thiết để đảm bảo tính chính xác, nhưng chúng cũng làm tăng chi phí tính toán.

# 3. Cân nhắc về kích thước [[ô|ô nhớ]] ([[khối luồng]])
- Mỗi [[khối luồng]] cần có nhiều [[luồng]]:
	- Tile_width = 16 cho 16 x 16 = 256 [[luồng]].
	- Tile_width = 32 cho 32 x 32 = 1024 [[luồng]].
- Đối với 16, trong mỗi [[pha|giai đoạn]], mỗi khối thực hiện:
	- 2 x 256 = 512 tải nổi từ [[bộ nhớ toàn cục]].
	- Tổng số phép toán tương ứng: **256 × (2 × 16) = 8.192 phép toán nhân hoặc cộng**. (Đây là số phép toán [[dấu phẩy động]], với mỗi lần tải từ bộ nhớ bao gồm **16 phép toán [[dấu phẩy động]]**).
-> Cần cân nhắc kích thước tối ưu của **Tile_width** dựa trên khả năng [[phần cứng]], [[băng thông]] [[bộ nhớ]] và [[mức độ song song]] của ứng dụng.
-> Việc tối ưu hóa **[[khối luồng]]** không chỉ ảnh hưởng đến [[hiệu suất]] tổng thể mà còn giúp giảm thiểu [[xung đột truy cập]] [[bộ nhớ toàn cục]].

# 4. [[Bộ nhớ chia sẻ]] và [[luồng]]

Trong [[lập trình]] [[GPU]], việc [[tối ưu hóa]] [[bộ nhớ chia sẻ]] và bố trí luồng trong mỗi [[khối]] (thread block) có vai trò quan trọng trong việc tăng [[hiệu năng]] tính toán. Ví dụ dưới đây giả sử một [[SM]] có 16 KB bộ nhớ chia sẻ.

### 1. Bộ nhớ chia sẻ và cách tính toán sử dụng

- **Kích cỡ bộ nhớ chia sẻ** được sử dụng phụ thuộc vào thiết kế kernel và việc thực thi. Thông số 16 KB có thể là giới hạn vật lý của SM, nhưng cách phân bổ tùy thuộc vào yêu cầu của kernel.
    

#### Với TILE_WIDTH = 16

- **Số luồng mỗi khối:** Với TILE_WIDTH = 16, khối luồng được sắp xếp theo lưới 16×16, tức có **256 luồng/khối**.
    
- **Bộ nhớ chia sẻ sử dụng cho mỗi khối:** Giả sử mỗi luồng cần tải dữ liệu từ bộ nhớ toàn cục với 2 lần tải (2 phép toán), mỗi lần tải dữ liệu kích thước 4 Byte. Khi đó:
    

Bộ nhớ chia sẻ=2×256×4 Byte=2 KB\text{Bộ nhớ chia sẻ} = 2 \times 256 \times 4\,\text{Byte} = 2\,\text{KB}

- **Số khối luồng hoạt động đồng thời:** Với 16 KB bộ nhớ chia sẻ, số khối tối đa có thể chạy đồng thời là:
    

16 KB/2 KB=8 khoˆˊi luoˆˋng16\,\text{KB} / 2\,\text{KB} = 8\,\text{khối luồng}

- **Tải dữ liệu đang chờ xử lý:** Mỗi khối có 256 luồng với 2 lần tải, do đó:
    

8 khoˆˊi×(256×2)=4096 tải8\,\text{khối} \times (256 \times 2) = 4096\,\text{tải}

Điều này cho thấy khả năng xử lý song song lớn, giúp che giấu thời gian truy cập bộ nhớ.

#### Với TILE_WIDTH = 32

- **Số luồng mỗi khối:** Với TILE_WIDTH = 32, khối luồng được sắp xếp theo lưới 32×32, tức có **1024 luồng/khối**.
    
- **Bộ nhớ chia sẻ sử dụng cho mỗi khối:** Tương tự, mỗi luồng cần thực hiện 2 lần tải với mỗi phép tải 4 Byte:
    

Bộ nhớ chia sẻ=2×32×32×4 Byte=8192 Byte=8 KB\text{Bộ nhớ chia sẻ} = 2 \times 32 \times 32 \times 4\,\text{Byte} = 8192\,\text{Byte} = 8\,\text{KB}

- **Số khối luồng hoạt động đồng thời:** Với 16 KB bộ nhớ chia sẻ, về lý thuyết có thể chạy đồng thời:
    

16 KB/8 KB=2 khoˆˊi luoˆˋng16\,\text{KB} / 8\,\text{KB} = 2\,\text{khối luồng}

Tuy nhiên, cần lưu ý rằng một số GPU giới hạn số luồng tối đa trên mỗi SM. Ví dụ, nếu giới hạn là 1536 luồng/SM, thì:

- Với mỗi khối chứa 1024 luồng, việc chạy 2 khối sẽ yêu cầu 2048 luồng, vượt quá giới hạn.
    
- Như vậy, chỉ **1 khối** có thể chạy đồng thời, ngay cả khi bộ nhớ chia sẻ cho phép chạy 2 khối.
    

### 2. Ảnh hưởng của đồng bộ hóa với `__syncthreads()`

- `__syncthreads()` là lệnh bắt buộc tất cả các luồng trong khối dừng lại tại điểm đồng bộ, đảm bảo rằng mọi luồng đều hoàn thành phần xử lý của mình trước khi đi tiếp.
    
- Mỗi lần gọi `__syncthreads()` có thể làm giảm hiệu quả thực thi đồng thời do buộc các luồng phải chờ luồng chậm nhất hoàn thành công việc.
    
- **Lời khuyên:** Nếu kernel có nhiều lần đồng bộ hóa, hãy cân nhắc chia nhỏ khối luồng thành nhiều khối nhỏ (nhiều khối) thay vì một khối lớn, từ đó tối ưu hoá hiệu năng nhiệm vụ, đặc biệt trong bối cảnh bộ nhớ chia sẻ bị giới hạn.
    

### 3. Tổng kết

- **TILE_WIDTH = 16:**
    
    - Mỗi khối sử dụng 2 KB bộ nhớ chia sẻ.
        
    - Có thể có đến 8 khối chạy đồng thời, tạo ra 4096 lượt tải dữ liệu đồng thời.
        
- **TILE_WIDTH = 32:**
    
    - Mỗi khối sử dụng 8 KB bộ nhớ chia sẻ.
        
    - Về lý thuyết, có thể có 2 khối chạy đồng thời, nhưng do giới hạn số luồng (ví dụ 1536 luồng/SM), chỉ có thể chạy được 1 khối đồng thời.
        
- **Đồng bộ hóa (**`__syncthreads()`**):**
    
    - Mỗi lần đồng bộ hóa có thể giảm bớt số luồng hoạt động hiệu quả trong khối.
        
    - Việc có nhiều khối nhỏ thay vì một khối lớn có thể giúp cải thiện hiệu năng tổng thể bằng cách giảm tải đồng bộ nội bộ.