# Mục tiêu
- Học cách sử dụng hiệu quả các loại [[bộ nhớ]] [[CUDA]] trong chương trình song song.
- Phân tích tầm quan trọng của hiệu quả truy cập bộ nhớ và phân tích cách thức thông lượng truy cập bộ nhớ có thể là một yếu tố giới hạn thông lượng thực thi của một bộ xử lý song song lớn. 
- Xem xét các thanh ghi (Register), bộ nhớ chia sẻ (shared memory) và bộ nhớ toàn cục (global memory) với trọng tâm vào phạm vi và thời gian tồn tại của các biến được khai báo trong các loại bộ nhớ này. 
# Kernel làm mờ hình ảnh - Image Blur Kernel trong lập trình CUDA
   
![[Ảnh màn hình 2025-03-13 lúc 20.15.01.png]]

Bên trên là đoạn code dùng trong CUDA để làm mờ một hình ảnh bằng cách tính trung bình các pixel xung quanh một điểm ảnh cụ thể. 
- Mục đích: Làm mờ một ảnh bằng cách thay thế mỗi pixel bằng trung bình của các pixel xung quanh trong một vùng có kích thước (2×BLUR_SIZE+1)×(2×BLUR_SIZE+1) 
- Cách hoạt động: 
1. Duyệt qua vùng xung quanh: 
   Hai vòng lặp `for` quét qua vùng lân cận có kích thước (2×BLUR_SIZE+1)×(2×BLUR_SIZE+1)(2×BLUR_SIZE+1)×(2×BLUR_SIZE+1) quanh mỗi pixel trung tâm.
2. Tính toán vị trí mới: 
   `curRow` và `curCol` là tọa độ của các pixel lân cận.
3. Kiểm tra hợp lệ: 
   Điều kiện `if` đảm bảo chỉ lấy giá trị của những pixel nằm trong biên ảnh (tránh truy cập ngoài vùng nhớ).
4. Tính tổng giá trị pixel: 
   Nếu pixel hợp lệ, cộng giá trị vào `pixVal` và tăng biến đếm `pixels`.
5. Gán giá trị pixel mới: 
   Sau khi duyệt qua tất cả các pixel lân cận, lấy giá trị trung bình và gán vào ảnh kết quả
# Vấn đề hiệu năng trên GPU ? 
#### 1. Vấn đề truy cập bộ nhớ toàn cục (Global Memory)
   - Mỗi thread truy cập vào bộ nhớ toàn cục để lấy dữ liệu của ma trận đầu vào. 
   - Mỗi phép cộng số thực dấu phẩy động (floating-point addition) cần một lần truy cập bộ nhớ, và mỗi lần truy cập tốn 4 byte dữ liệu.
   - Tỷ lệ băng thông bộ nhớ trên FLOPS (4B/s trên FLOP) ảnh hưởng đến hiệu năng thực thi. 
#### 2. Giả sử GPU có các thông số sau: 
   - **Tốc độ tính toán tối đa**: 1600 GFLOPS (tức là 1600 tỷ phép tính dấu chấm động mỗi giây).
   - **Băng thông bộ nhớ DRAM**: 600 GB/s.
   - Để đạt được **FLOPS tối đa**, GPU cần đọc dữ liệu với tốc độ:  4×1,600=6,400 GB/s nhưng băng thông thực tế chỉ có **600 GB/s**.
   - Do giới hạn băng thông, GPU chỉ có thể thực thi ở **150 GFLOPS**.
   ==> Tỷ lệ hiệu năng thực tế so với lý thuyết: 150 / 1600 = 9.3%
   ==> GPU chỉ hoạt động với **9.3% công suất tối đa** do bị giới hạn bởi băng thông bộ nhớ.
   ==> Cần giảm đáng kể số lần truy cập bộ nhớ để tiến gần đến mức 1600 GFLOPS
#### 3. Kết luận: 
   - GPU có tốc độ tính toán rất cao, nhưng nếu **truy cập bộ nhớ không hiệu quả**, hiệu năng thực tế bị giảm mạnh.
   - Để cải thiện hiệu suất, cần **tối ưu việc truy cập bộ nhớ** để giảm thiểu tắc nghẽn băng thông.
# Ví dụ: Phép nhân ma trận

![[Ảnh màn hình 2025-03-14 lúc 00.16.37.png]]

- Hình trên là một ví dụ cơ bản - phép nhân ma trận để minh hoạ việc sủ dụng hiệu quả các loại bộ nhớ. Sơ đồ cho thấy cách chia ma trận thành các khối với BLOCK_WIDTH và thực hiện tính toán song song. Trong đó: 
	  - M, N: hai ma trận đầu vào.
	  - P: Kết quả phép nhân ma trận P = M x N
- Để tính toán ma trận đầu ra P, mỗi phần tử của ma trận đầu ra P là tích vô hướng của một hàng của M, như hình minh hoạ đang hiển thị theo hướng ngang của mũi tên, và một cột của N, như hình minh hoạ đang hiển thị trong dải dọc và mũi tên dọc trong slide này.
# Kernel nhân ma trận

![[Ảnh màn hình 2025-03-14 lúc 01.14.05.png]]

- Hình trên là một kernel nhân ma trận cơ bản. Vì chúng ta đang tạo ra một mảng đầu ra P hai chiều, chúng ta có thể chỉ cần một luồng để tạo ra một phần tử của mảng đầu ra P. Điều này tương tự như kernel làm mờ hình ảnh (Image Blur Kernel).
- Trong đoạn mã này, chúng ta tính toán chỉ số hàng và chỉ số cột cho phần tử P mà một luồng cụ thể chịu trách nhiệm bằng cách sử dụng các giá trị `blockIdx`, `blockDim`, và `threadIdx`. Chỉ số hàng `Row` được tính bằng `blockIdx.y * blockDim.y + threadIdx.y`, và chỉ số cột `Col` được tính bằng `blockIdx.x * blockDim.x + threadIdx.x`.
- Mỗi luồng sẽ tính toán một phần tử của P bằng cách thực hiện một tích vô hướng giữa một hàng của M và một cột của N. Vòng lặp `for` thực hiện phép nhân từng phần tử tương ứng của hàng `Row` trong M và cột `Col` trong N, sau đó cộng dồn vào `Pvalue`. Cuối cùng, giá trị này được lưu vào phần tử `P[Row * Width + Col]`.

![[Ảnh màn hình 2025-03-14 lúc 01.23.52.png]]

- Đây là tính toán cốt lõi của kernel nhân ma trận cho mỗi luồng. Chúng ta thấy mẫu quen thuộc trong đó mỗi luồng sẽ kiểm tra xem chỉ số hàng và chỉ số cột của nó có nằm trong phạm vi hợp lệ của ma trận đầu ra P hay không bằng điều kiện `if ((Row < Width)  &&  (Col < Width))`.
- Một khi luồng xác định rằng nó nằm trong phạm vi hợp lệ của P, nó sẽ thực hiện tích vô hướng. Cụ thể, nó truy cập từng phần tử trong hàng `Row` của ma trận M và phần tử tương ứng trong cột `Col` của ma trận N. Trong vòng lặp `for`, mỗi cặp phần tử từ M và N được nhân với nhau, và kết quả được cộng dồn vào biến `Pvalue`, đóng vai trò là bộ tích lũy.
- Sau khi vòng lặp `for` lặp qua tất cả các phần tử của hàng M và cột N, giá trị tích lũy cuối cùng của `Pvalue` chính là giá trị của phần tử P(Row, Col). Cuối cùng, nó được ghi vào vị trí tương ứng trong ma trận P. Đây là bước hoàn thiện việc tính toán của kernel cho mỗi luồng, giúp đảm bảo rằng mỗi phần tử của P được tính toán một cách độc lập nhưng vẫn đúng với quy tắc nhân ma trận.)
# Ví dụ:  Ánh xạ dữ liệu từ các luồng tới ma trận kết quả P

![[Ảnh màn hình 2025-03-14 lúc 01.31.11.png]]

- Ở đây, chúng ta sử dụng một ví dụ đơn giản để minh họa cách ánh xạ các luồng đến phần tử dữ liệu trong ma trận kết quả P.
- Chúng ta đang làm việc với một khối luồng rất nhỏ có kích thước 2×2, và một lưới cũng rất nhỏ có kích thước 2×2. Với cấu hình này, chúng ta có thể thực hiện phép nhân hai ma trận có kích thước 4×4, tạo ra ma trận kết quả cũng có kích thước 4×4.
- Trong hình, chúng ta thấy rằng khối (0,0) ánh xạ đến góc trên bên trái của ma trận kết quả, bao gồm các phần tử P(0,0), P(0,1), P(1,0), P(1,1)​. Trong khối (0,0), mỗi luồng đảm nhận một phần tử: luồng (0,0) ánh xạ đến P(0,0)​, luồng (1,0) ánh xạ đến P10, v.v.
- Tương tự, trong khối (1,1), luồng (0,0) của khối ánh xạ đến phần tử P(2,2)​, còn luồng (1,1) trong khối này ánh xạ đến phần tử P(3,3).
- Khái niệm chiều rộng khối (BLOCK_WIDTH) được sử dụng để biểu thị kích thước của khối theo cả hai hướng x và y. Trong trường hợp này, BLOCK_WIDTH = 2, nghĩa là mỗi khối có kích thước 2×2.
- Lưu ý: Trong các bài toán thực tế, ma trận không nhất thiết phải là vuông, và đối với các ma trận hình chữ nhật, cách tiếp cận tương tự vẫn có thể áp dụng. Tuy nhiên, khi đó, chúng ta cần chỉ rõ kích thước chiều rộng theo cả hai hướng trong kernel để đảm bảo tính chính xác của phép nhân ma trận.

![[Ảnh màn hình 2025-03-14 lúc 01.43.21.png]]

- Hình này minh họa cách tính toán các phần tử P(0,0)​ và P(0,1)​ trong ma trận kết quả.
- Để tính P(0,0)​, chúng ta thực hiện tích vô hướng giữa hàng 0 của ma trận M và cột 0 của ma trận N. Điều này được thể hiện bởi hai mũi tên: một ngang đi qua hàng 0 của M và một dọc đi qua cột 0 của N.
- Tương tự, để tính P(0,1), chúng ta lấy hàng 0 của M nhưng kết hợp với cột 1 của N. Tập hợp mũi tên thứ hai minh họa điều này.
- Điểm quan trọng ở đây là cả hai phần tử P(0,0)​ và P(0,1)​ đều sử dụng cùng một hàng của M, nhưng các cột khác nhau của N. Điều này có ý nghĩa quan trọng đối với hiệu suất bộ nhớ khi thực hiện phép nhân ma trận trên GPU, vì nó cho thấy cách dữ liệu có thể được tải vào bộ nhớ và sử dụng lại để tăng hiệu quả tính toán.)
# Bộ nhớ và thanh ghi trong mô hình Von-Neumann

![[Ảnh màn hình 2025-03-14 lúc 01.53.14.png]]

- Hình này minh họa mô hình Von-Neumann, một kiến trúc máy tính cơ bản trong đó chương trình và dữ liệu được lưu trữ trong cùng một bộ nhớ.
- Các thành phần chính: 
  1. [[Bộ nhớ]] (Memory): 
     - Lưu trữ cả [[chương trình]] và [[dữ liệu]].
     - Được kết nối với [[Bộ xử lý]] (Processing Unit) để truy xuất dữ liệu và lệnh.
 2. [[Bộ xử lý]] (Processing Unit):  
    - Bao gồm [[Đơn vị số học và Logic]] - ALU (Arithmetic Logic Unit) để thực hiện các phép toán số học và logic. 
    - [[Tập thanh ghi]] (Reg File) giúp lưu trữ tạm thời dữ liệu để tăng tốc độ xử lý.
3. Bộ điều khiển (Control Unit):
    - Chứa Bộ đếm chương trình - PC (Program Counter)**, giúp theo dõi lệnh nào sẽ được thực thi tiếp theo.
    - Chứa Thanh ghi lệnh - IR (Instruction Register)**, giữ lệnh hiện tại đang được thực thi.
    - Điều khiển luồng dữ liệu giữa bộ xử lý, bộ nhớ và các thiết bị đầu vào/đầu ra.
4.  Thiết bị vào / Ra (IO):
	- Giao tiếp với các thiết bị bên ngoài để nhập/xuất dữ liệu.
# Góc nhìn của lập trình viên về các bộ nhớ CUDA

![[Ảnh màn hình 2025-03-14 lúc 02.26.55.png]]

- Hình này minh họa cách các lập trình viên có thể hiểu và quản lý bộ nhớ trong CUDA, cung cấp cái nhìn tổng quan về các loại bộ nhớ khác nhau và cách chúng được sử dụng trong quá trình tính toán song song.
#### 1. Bộ nhớ toàn cục (Global Memory) và Bộ nhớ hằng số (Constant Memory):
   - Bộ nhớ Toàn cục (Global Memory) là vùng nhớ chung của toàn bộ hệ thống (ô màu cam, dưới cùng).
   - Bộ nhớ Hằng số (Constant Memory) cũng là một vùng nhớ toàn cục, nhưng được tối ưu cho truy cập đọc, giúp các luồng truy xuất dữ liệu nhanh hơn nếu dữ liệu được dùng chung nhiều lần.
   - Kết nối với Host: Cả hai loại bộ nhớ này đều có thể được truy cập từ máy chủ (Host), như thể hiện qua các mũi tên giữa Host và Global/Constant Memory.
#### 2. Bộ nhớ chia sẻ (Shared Memory):
  -  Bộ nhớ chia sẻ được cấp phát riêng cho từng Khối (vùng màu cam trong từng [[Khối]]).
   - Bộ nhớ chia sẻ có thể truy cập bởi tất cả các Thread trong cùng khối như được thể hiện bằng các mũi tên dọc giữa luồng (Thread) và bộ nhớ này.
   - Đây là một vùng nhớ nhanh hơn bộ nhớ toàn cục, giúp các luồng trong cùng một khối giao tiếp với nhau hiệu quả hơn.
#### 3. Thanh ghi (Registers):
  -  Mỗi Thread (luồng) (ô màu xanh lá cây) có một bộ thanh ghi (Registers) riêng (màu cam) để lưu trữ giá trị trung gian khi thực hiện tính toán.
  - Thanh ghi là vùng nhớ nhanh nhất mà mỗi luồng có thể truy cập trực tiếp, điều này được thể hiện qua các mũi tên nhỏ giữa Thread và Registers trong mỗi khối.
# Khai báo các biến trong CUDA

![[Ảnh màn hình 2025-03-14 lúc 02.53.48.png]]

- Slide này trình bày các cách khác nhau để khai báo biến trong CUDA và cách chúng được ánh xạ vào các loại bộ nhớ cụ thể.
#### 1. Biến cục bộ trong thanh ghi (Registers): 
   - Cú pháp: int localVar;
   - Loại bộ nhớ: register
   - Phạm vi: Chỉ trong luồng (thread) khai báo biến đó.
   - Thời gian tồn tại: Biến tồn tại chỉ trong thời gian thực thi của luồng và sẽ bị giải phóng ngay khi luồng kết thúc.
   - Ghi chú: Biến tự động trong CUDA thường được đặt trong thanh ghi nếu có thể. Nhưng nếu nó là một mảng (array), nó sẽ được đặt trong bộ nhớ toàn cục thay vì thanh ghi.
#### 2. Biến bộ nhớ chia sẻ (Shared Memory):
   - Cách khai báo: _ _device__ __shared_ _ int SharedVar; 
   - Loại bộ nhớ: Shared Memory (Bộ nhớ chia sẻ).
   - Phạm vi (Scope): Chỉ có thể truy cập trong phạm vi khối, nghĩa là tất cả các luồng trong cùng một khối có thể đọc/ghi biến này.
   - Thời gian tồn tại: Biến tồn tại trong suốt thời gian thực thi của khối, và sẽ bị xóa khi khối hoàn thành công việc.
   - Lưu ý quan trọng: Các luồng trong cùng khối có thể nhìn thấy và chia sẻ bộ nhớ này, nhưng không đảm bảo rằng chúng sẽ thấy giá trị cập nhật ngay lập tức. Để đồng bộ hóa việc đọc và ghi vào Bộ nhớ chia sẻ (Shared Memory), chúng ta cần sử dụng cơ chế __syncthreads().
#### 3. Biến bộ nhớ toàn cục (Global Memory): 
   - Khai báo: _ __ _device_ _ _ int GlobalVar;
   - Bộ nhớ: Global Memory (Bộ nhớ toàn cục)
   - Phạm vi (Scope): Được chia sẻ trên toàn bộ Lưới. Điều này có nghĩa là mọi luồng trong tất cả các khối có thể truy cập cùng một biến.
   - Thời gian tồn tại: Suốt vòng đời của ứng dụng (Application).
   - Lưu ý quan trọng: Bộ nhớ toàn cục có độ trễ cao hơn Bộ nhớ chia sẻ và Thanh ghi. Do đó, truy cập dữ liệu từ Bộ nhớ toàn cục có thể mất nhiều thời gian và yêu cầu các chiến lược đồng bộ phù hợp để đảm bảo tính nhất quán dữ liệu.
#### 4. Biến hằng số (Constant Memory):
   - Bộ nhớ: Constant Memory (Bộ nhớ Hằng số)
   - Phạm vi (Scope): Tương tự Bố nhớ toàn cục, bộ nhớ hằng số có thể được truy cập bởi tất cả các luồng trong lưới (grid).
   - Thời gian tồn tại: Suốt vòng đời của ứng dụng (Application).
   - Lưu ý quan trọng: Chỉ có một bản duy nhất của biến hằng số trong toàn bộ lưới. Biến này chỉ đọc được bởi các luồng (read-only), nhưng có thể được khởi tạo từ phía máy chủ -  host (CPU). Vì chỉ có một phiên bản của biến này, dữ liệu trong bộ nhớ hằng số không bị thay đổi bởi các luồng trong kernel.
#### - Ghi chú: 
	1.  Từ khóa __device__ là tùy chọn khi khai báo biến __shared__ hoặc __constant__.
	2.  Các biến tự động (automatic variables) được đặt trong các thanh ghi, ngoại trừ:
	- Mảng trong mỗi luồng (per-thread arrays) sẽ được đặt vào bộ nhớ toàn cục (global memory) thay vì thanh ghi.
	3. Các khác biệt chính giữa bộ nhớ toàn cục và bộ nhớ chia sẻ:
    - Bộ nhớ toàn cục (Global Memory): Chia sẻ trong toàn bộ lưới (grid). Các threads từ các blocks khác nhau có thể chia sẻ dữ liệu từ đây.
    - Bộ nhớ chia sẻ (Shared Memory): Chỉ tồn tại trong block, chỉ các threads trong block đó mới truy cập được. Dữ liệu trong bộ nhớ chia sẻ không thể được nhìn thấy bởi các luồng từ block khác.
#### - Ví dụ: 

![[Ảnh màn hình 2025-03-14 lúc 03.20.51.png]]

- Bên trên là ví dụ nhanh về việc khai báo một biến bộ nhớ chia sẻ trong một kernel CUDA. 
- Sử dụng từ khóa **`__shared__`** để chỉ định bộ nhớ chia sẻ.
- Biến **`ds_in`** là một mảng hai chiều kiểu float, có kích thước được xác định bởi hằng số `TILE_WIDTH`.
- Vì `TILE_WIDTH` là một hằng số tại thời gian biên dịch, kích thước của mảng `ds_in` được xác định trước khi chạy kernel.
- Bộ nhớ chia sẻ giúp tăng tốc độ truy xuất dữ liệu trong kernel CUDA do có độ trễ thấp hơn so với Bộ nhớ toàn cục.
- Chỉ có các luồng trong cùng một khối mới có thể truy cập bộ nhớ chia sẻ này, giúp tăng tốc độ truy xuất dữ liệu so với bộ nhớ toàn cục.
# Đâu là vị trí thích hợp để khai báo các loại biến trong CUDA ?

![[Ảnh màn hình 2025-03-14 lúc 11.43.45.png]]

- Đây là một đánh giá nhanh về nơi chúng ta nên khai báo mỗi loại biến bộ nhớ. 
- Một cách nhanh chóng để xác định nơi đặt khai báo là hỏi liệu máy chủ có thể truy cập giá trị hay không. Vì vậy, máy chủ cần có khả năng truyền dữ liệu vào và ra khỏi bộ nhớ toàn cục và bộ nhớ hằng số thông qua các API. Đó là lý do tại sao *các **biến loại bộ nhớ toàn cục và bộ nhớ hằng số*** này cần được ***khai báo bên ngoài*** bất kỳ hàm kernel nào. 
    - Bộ nhớ toàn cục (`global memory`): 
        - Được truy cập bởi cả máy chủ (host) và thiết bị (device).
        - Tất cả các luồng (thread) trong một lưới (grid) có thể đọc/ghi vào nó.
        - Có độ trễ truy cập cao và băng thông hạn chế.
    - Bộ nhớ hằng số (`constant memory`): 
        - Dữ liệu chỉ đọc, có thể truy cập bởi tất cả threads.
        - Thường được dùng cho các hằng số được truyền từ máy chủ (host) đến thiết bị (device) một lần duy nhất.
- Trong khi đó, ***các thanh ghi (Registers) và bộ nhớ chia sẻ (Shared Memory)*** không thể truy cập được bởi máy chủ và chúng được các luồng truy cập một cách nghiêm ngặt. Và vì vậy, chúng sẽ ***được khai báo trong kernel.***
- Và sự khác biệt giữa hai loại là các biến bộ nhớ chia sẻ là trên cơ sở khối và các thanh ghi là riêng tư cho mỗi luồng. Và chúng ta sẽ thấy rằng việc sử dụng hiệu quả bộ nhớ chia sẻ thực sự sẽ cho phép chia sẻ dữ liệu và trao đổi dữ liệu giữa các luồng trong một khối luồng. 
    - Thanh ghi (Register): 
        - Biến riêng tư cho từng luồng (thread) (mỗi luồng có một bản sao của biến).
        - Truy cập rất nhanh, nhưng kích thước bộ nhớ giới hạn.
    - Bộ nhớ chia sẻ (Shared Memory):
        - Chỉ có thể truy cập được bởi các luồng (threads) trong cùng một khối (block).
        - Tăng hiệu suất khi nhiều thread trong cùng một khối cần truy cập chung một dữ liệu.
# Bộ nhớ chia sẻ trong CUDA
- Một loại bộ nhớ đặc biệt mà nội dung của nó được định nghĩa và sử dụng rõ ràng trong mã nguồn kernel.
- Mỗi bộ xử lý đa luồng - SM (Streaming Multiprocessor) có một bộ nhớ chia sẻ.
- Được truy cập với tốc độ cao hơn nhiều (cả về độ trễ và thông lượng) so với bộ nhớ toàn cục.
- Phạm vi truy cập và chia sẻ - các khối luồng.
- Vòng đời - theo khối thread, nội dung sẽ biến mất sau khi thread tương ứng kết thúc thực thi
- Được truy cập bằng các lệnh tải/lưu trữ bộ nhớ.
- Một dạng bộ nhớ vùng đệm tạm thời (scratchpad memory) trong kiến trúc máy tính.
Góc nhìn phần cứng về các bộ nhớ CUDA